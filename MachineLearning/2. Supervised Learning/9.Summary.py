'''
                            Summary and Outlook
'''

'''
우리는 다음과 같은 개념들을 배웠다.
    1. model Complexity

    2. generalization

    3. underfitting

    4. overfitting


또한, 분류와 회귀를 위한 머신 러닝 알고리즘을 배우고, 다음과 같은 사항도 알게 되었다.
    1. 파라미터의 설정이 중요하다.

    2. input data를 잘 선정하는 것이 중요하다.

    3. input data의 전처리(preprocessing, rescaling)도 중요하다.


각 알고리즘을 언제 적용해야할까? 아래 요약이 있다.
    1. Nearest neighbors
        작은 데이터셋에 좋다.
        기초 알고리즘으로서 좋고 설명하기 쉽다.

    2. Linear models
        처음으로 시도할 모델로서 좋다.
        아주 큰 데이터셋에 좋고, 아주 큰 차원의 데이터에 좋다.

    3. Naive Bayes
        분류를 위해서만 쓰인다.
        선형 모델보다도 빠르고, 아주 큰 데이터셋과 아주 높은 차원의 데이터에 좋다.
        종종 선형 모델보다 덜 정확하다.

    4. Decision Trees
        아주 빠르고 데이터의 가공이 필요하지 않다.
        시각화와 설명이 쉽다.

    5. Random Forests
        거의 대부분 결정 트리보다 성능이 좋다.
        데이터의 가공이 필요하지 않다.
        고차원의 sparse 데이터셋에 부적합하다.

    6. Gradient Boosted decision trees
        종종 랜덤 포레스트보다 약간 더 성능이 좋다.
        학습이 느리지만 예측이 더 빠르다. (랜덤 포레스트에 비해)
        랜덤 포레스트보다 파라미터 조율이 더 필요하다.

    7. Support Vector Machines
        중간 사이즈의 데이터셋에 적합하다.
        feature들이 비슷한 성격을 띨 때 좋다.
        데이터의 가공이 필요하고, 파라미터에 민감하다.

    8. Neural networks
        아주 복잡한 모델을 만들 수 있다. (특히 큰 데이터셋에)
        데이터의 가공에 민감하고, 파라미터의 선택에 민감하다.
        큰 모델은 훈련에 시간이 오래 걸린다.

이 모델들을 가지고 하나의 데이터셋에 이리저리 맞춰보며 실험해보는 것이 공부하는 데 도움이 된다. scikit-learn에는 자체 데이터셋이 많으므로 해보도록 하자.


새로운 데이터셋을 마주했을 때, 다음의 방법을 따르는 것이 좋다.
    1. 간단한 모델부터 시작한다. 그리고 어느 정도 성능을 내는지 지켜본다.
        -선형 모델, 나이브 베이즈, kNN

    2. 데이터에 대한 이해를 바탕으로, 더 복잡한 모델로 나아간다.
        -랜덤 포레스트, 그래디언트부스팅, SVM, 뉴럴 네트워크

    3. 파라미터와 그 외 변수들을 조율해본다.
'''
