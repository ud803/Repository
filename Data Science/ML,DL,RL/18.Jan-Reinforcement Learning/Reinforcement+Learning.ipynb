{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://mpatacchiola.github.io/blog/2016/12/09/dissecting-reinforcement-learning.html\n",
    "\n",
    "### RL의 기원을 알기 위해서는, 먼저 마코브 체인부터 짚고 넘어가야 한다.\n",
    "\n",
    "`Andrey Markov`는 **확률 과정(Stochastic Process)**을 공부했던 러시아 수학자였다.\n",
    "\n",
    "그는 특히 연결되어 있는 사건들에 대해 관심이 많았는데, 1906년 그는 '**체인(Chain)**' 이라고 불렀던 개별 과정에 대해 관심을 갖기 시작했다.\n",
    "\n",
    "**마코브 체인(Markov Chain)**은 State 집합 $ S = \\{s_0, s_1, ... s_m\\} $과 한 state에서 다른 state으로 이동할수 있는 process를 가지고 있다.\n",
    "\n",
    "이러한 이동은 한 단계에 거쳐 발생하며, **전이 모델(Transition Model)**인 $T$에 기초한다.\n",
    "\n",
    "즉, 마코브 체인은 다음 3가지로 정의할 수 있다.\n",
    "\n",
    "\n",
    "1. Set of Possible States : $ S = \\{s_0, s_1, ..., s_m\\} $\n",
    "2. Initial States : $s_0$\n",
    "3. Transition Model : $T(s, s')$\n",
    "\n",
    "또한 마코브 체인은 **마코브 성질(Markov Property)**에 기반하는데, 현재가 주어졌을 때, 미래의 상황은 과거와는 조건부 독립이라는 것이다. 즉, 현재 프로세스가 속한 `state`은 직전 상황에 속해있던 `state`에만 영향을 받는다는 말이다. \n",
    "\n",
    "예를 들어 살펴보자.\n",
    "\n",
    "두 개의 `state` : $s_0, s_1$이 있고, $s_0$가 초기 `state`라고 생각하자.\n",
    "\n",
    "**전이 행렬(Transition Matrix)**은 다음과 같이 정의된다.\n",
    "\n",
    "$$ T =  \\left[ \\begin{array}{cc} 0.90 & 0.10 \\\\ 0.50 & 0.50  \\end{array} \\right] $$\n",
    "\n",
    "\n",
    "![title](./Markov.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 시간의 개념을 도입해보자. \n",
    "\n",
    "3단계 이후, 50단계 이후에 프로세스가 어떤 `state`에 있을지 예측한다고 해보자.\n",
    "\n",
    "k-step 전이 확률(Transition Probability)를 구하려면 그냥 전이행렬을 k차 제곱하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T: [[ 0.9  0.1]\n",
      " [ 0.5  0.5]]\n",
      "T_3: [[ 0.844  0.156]\n",
      " [ 0.78   0.22 ]]\n",
      "T_50: [[ 0.83333333  0.16666667]\n",
      " [ 0.83333333  0.16666667]]\n",
      "T_100: [[ 0.83333333  0.16666667]\n",
      " [ 0.83333333  0.16666667]]\n"
     ]
    }
   ],
   "source": [
    "# 전이행렬 계산하기\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "T = np.array([[0.9, 0.1], \n",
    "             [0.5, 0.5]])\n",
    "\n",
    "T_3 = np.linalg.matrix_power(T, 3)\n",
    "T_50 = np.linalg.matrix_power(T, 50)\n",
    "T_100 = np.linalg.matrix_power(T, 100)\n",
    "\n",
    "print(\"T: \" + str(T))\n",
    "print(\"T_3: \" + str(T_3))\n",
    "print(\"T_50: \" + str(T_50))\n",
    "print(\"T_100: \" + str(T_100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 k=0에서 `state`를 나타내는 초기 분포(initial distribution)를 정의한다.\n",
    "\n",
    "이 시스템은 2개의 `state`으로 구성되어 있기에 초기 분포를 길이가 2인 벡터로 나타낼ㄹ 수 있다.\n",
    "\n",
    "첫 번째 원소는 $s_0$에 있을 확률이고, 두 번째 원소는 $s_1$에 있을 확률이다.\n",
    "\n",
    "우리가 $s_0$부터 시작한다면, 초기 분포는 다음과 같다.\n",
    "\n",
    "$$ v = (1,0) $$\n",
    "\n",
    "이를 반영하여 다시 전이 확률을 계산할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v: [[ 1.  0.]]\n",
      "v_1: [[ 0.9  0.1]]\n",
      "v_3: [[ 0.844  0.156]]\n",
      "v_50: [[ 0.83333333  0.16666667]]\n",
      "v_100: [[ 0.83333333  0.16666667]]\n"
     ]
    }
   ],
   "source": [
    "# 초기 확률이 (1, 0)\n",
    "v = np.array([[1.0, 0.0]])\n",
    "\n",
    "print(\"v: \" + str(v))\n",
    "print(\"v_1: \" + str(np.dot(v,T)))\n",
    "print(\"v_3: \" + str(np.dot(v,T_3)))\n",
    "print(\"v_50: \" + str(np.dot(v,T_50)))\n",
    "print(\"v_100: \" + str(np.dot(v,T_100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v: [[ 0.5  0.5]]\n",
      "v_1: [[ 0.7  0.3]]\n",
      "v_3: [[ 0.812  0.188]]\n",
      "v_50: [[ 0.83333333  0.16666667]]\n",
      "v_100: [[ 0.83333333  0.16666667]]\n"
     ]
    }
   ],
   "source": [
    "# 초기 확률이 (0.5, 0.5)\n",
    "v = np.array([[0.5, 0.5]])\n",
    "\n",
    "print(\"v: \" + str(v))\n",
    "print(\"v_1: \" + str(np.dot(v,T)))\n",
    "print(\"v_3: \" + str(np.dot(v,T_3)))\n",
    "print(\"v_50: \" + str(np.dot(v,T_50)))\n",
    "print(\"v_100: \" + str(np.dot(v,T_100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "놀라운 점은, 시간이 지나면 초기 확률에 관계 없이 모두 동일한 전이 확률을 갖게 된다는 사실이다.\n",
    "\n",
    "결국 체인은 평형 상태로 수렴했고, 초기 분포는 잊혀졌다. \n",
    "\n",
    "이러한 **수렴(Convergence)**이 항상 일어나는 것은 아니므로 조심해야 한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 마코브 의사결정 프로세스 (Markov Decision Process)\n",
    "\n",
    "RL에서, 마코브 체인과 비슷한 개념인 **MDP(Markov Decision Process)**가 자주 사용된다. \n",
    "\n",
    "MDP는 마코브 체인을 재해석한 것으로, `agent`와 `decision making`의 개념을 포함한다.\n",
    "\n",
    "MDP는 다음 5가지 개념으로 정의된다.\n",
    "\n",
    "1. Set of possible states : $ S = \\{s_0, s_1, ..., s_m\\} $\n",
    "2. Initial State : $s_0$\n",
    "3. Set of possible Actions : $ A = \\{a_0, a_1, ..., a_n\\} $\n",
    "4. Transition Model : $ T(s, a, s') $\n",
    "5. Reward Function: $ R(s) $\n",
    "\n",
    "마코브 체인과 비교하여 추가적인 요소들이 생겨났다.\n",
    "\n",
    "- 전이 모형에 초기 상태인 $s_0$가 추가되었고, 이 전이모형은 행동 `a`가 상태 `s`에서 수행되었을 때 `s'`에 도달할 확률을 계산한다.\n",
    "\n",
    "- 그리고 보상 함수인 R(s)가 추가되었는데, 이 함수 덕분에 몇몇 상태가 다른 것보다 더 바람직하다라고 말할 수 있다. \n",
    "\n",
    "- 따라서 행동 주체가 그러한 상태로 움직이면 더 높은 보상을 받는다.\n",
    "\n",
    "그렇다면 여기서의 목표와 해결법은 다음과 같이 말할 수 있다.\n",
    "\n",
    "- Problem : 행동 주체는 '-'값을 주는 상태를 피하고, '+'값을 주는 상태를 고르면서 보상을 극대화시켜야한다.\n",
    "- Solution : 가장 높은 보상을 주는 행동을 반환하는 '**규칙(policy)**'-  $\\pi (s)$를 찾아야한다.\n",
    "\n",
    "\n",
    "행동 주체(agent)는 서로 다른 규칙들을 시도해볼 수 있지만, 그 중 가장 높은 기대 효용값을 산출하는 하나만이 **최적 규칙(Optimized Policy)**으로 인정되고, $ \\pi^*$으로 정의된다.\n",
    "\n",
    "\n",
    "\n",
    "### 이제 한 예제를 살펴볼 것인데, 앞으로도 계속해서 쓰일 것이다.\n",
    "\n",
    "\n",
    "이 실험은 `Russel and Norving`의 책 챕터 17.1에 소개되어 있다.\n",
    "\n",
    "- 우리에게 로봇 청소기가 있고, 이 청소기는 반드시 경로에서 충전소를 지나가야 한다고 가정해보자.\n",
    "\n",
    "- 로봇에게 주어진 공간은 4x3 행렬이며, 출발지점 $s_0$는 (1,1)에 위치한다. \n",
    "- 충전소는 (4,3)에 위치하고, 위험한 계단은 (4,2)에, 장애물은 (2,2)에 위치한다.\n",
    "- 로봇은 충전소(Reward +1)에 도달하고, 계단(Reward -1)을 피하는 최적의 경로를 찾아야 한다.\n",
    "- 확률 과정에 방해물이 개입하여, 원래 의도된 경로에서 항상 20%정도 벗어난다.\n",
    "- 만약 로봇이 앞으로 나아가기로 결정할 때, 10% 확률로 왼쪽을 바라보고, 10% 확률로 오른쪽을 바라본다.\n",
    "- 만약 로봇이 벽이나 장애물을 만나면, 다시 원래 자리로 돌아온다.\n",
    "\n",
    "![title](./Robot.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그렇다면 여기서 로봇이 택할 수 있는 **최적의 경로**는 무엇일까?\n",
    "\n",
    "배터리 수준에 따라서, 각 단계에서의 보상을 다르게 준다고 생각해보자. \n",
    "\n",
    "아래 4가지 경우를 가정할 수 있다.\n",
    "\n",
    "1. Extremely Low Battery : $R(s) \\leq -1.6284 $ \n",
    "    - 행동 주체(청소기)는 한 단계를 움직일 때마다 너무도 큰 벌을 받는다.\n",
    "    - 따라서 당장의 목표는 한시라도 빨리 모든 움직임을 멈추는 것이다.\n",
    "    - 결국 로봇 청소기는 계단으로 굴러 떨어지는 것을 선택한다.\n",
    "2. Quite Low Battery : $-0.4278 \\leq R(s) \\leq -0.085 $\n",
    "    - 청소기는 충전소에 최단 경로로 가려고 한다.\n",
    "3. Slightly Low Battery : $ -0.0221 \\leq R(s) \\leq 0 $\n",
    "    - 청소기는 위험을 감수하지 않고, 계단을 가로지르지 않는다.\n",
    "4. Fully Charged : $ R(s) > 0 $\n",
    "    - 청소기는 exit을 하지 않고, 매 단계에서 '+'의 보상을 받으며 안정 상태에 돌입한다.\n",
    "    \n",
    "\n",
    "그럼 로봇청소기는 **어떻게** 최적의 경로를 선택할까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Bellman Equation\n",
    "\n",
    "위 질문에 답하기 위해, **[벨만 방정식](https://en.wikipedia.org/wiki/Bellman_equation)**을 짚고 넘어가야 한다.\n",
    "\n",
    "먼저 우리는 '2개의 규칙'을 비교할 줄 알아야 한다.\n",
    "각 `state`에서 주어진 보상을 사용하여 `state sequence`의 효용을 계산할 수 있다.\n",
    "\n",
    "`states history` h의 효용을 다음과 같이 정의한다 :\n",
    "$$ U_h = R(s_0) + \\gamma R(s_1) + \\gamma^2 R(s_2) + ... + \\gamma^n (s_n) $$\n",
    "\n",
    "이 공식은 `state sequence`의 할인된 보상을 정의하는데, 여기서 $\\gamma \\in [0, 1]$은 **할인율(Discount Factor)**이다. \n",
    "\n",
    "할인율은 행동 주체가 '미래' 보상보다 '현재' 보상을 선호하는 정도를 나타낸다. 만약 할인율이 1이면 이 공식은 전체 합이 된다.\n",
    "\n",
    "`state`이 무한이 존재하는 경우에도, 위 공식을 사용하면 유한한 효용값을 얻을 수 있다. \n",
    "\n",
    "또한 단일 `state`의 효용은 다음과 같이 정의한다 :\n",
    "$$ U(s) = E \\left[ \\sum_{t=0}^\\infty \\gamma^t R(s_t) \\right]$$\n",
    "\n",
    "\n",
    "이제 모든 효용함수를 알았다. 그럼 다음 `state`을 위한 최선의 행동은 무엇일까?\n",
    "\n",
    "이 때 **Maximum Expected Utility(최대기대효용)** 원칙을 사용한다.\n",
    "\n",
    "\n",
    "\n",
    "##### -Bellman Equation\n",
    "$$ U(s) = R(s) + \\gamma max(a) \\sum_{s'}T(s, a, s') U(s') $$\n",
    "\n",
    "R(s)는 현재 state의 보상을 의미한다. \n",
    "T * U(s')는 모든 가능한 다음 state에 대해 확률을 구한다.\n",
    "\n",
    "하지만 이 방정식 만으로 해결이 되지 않는다. 유틸리티 함수 안에 또 다른 유틸리티 함수가 들어있기 때문이다.\n",
    "\n",
    "![title](./Robot-2.png)\n",
    "\n",
    "<br><br>\n",
    "\n",
    "\n",
    "![title](./Robot-3.png)\n",
    "\n",
    "<br><br>\n",
    "\n",
    "\n",
    "마지막으로 모든 값을 계산하면 아래 그림과 같다.\n",
    "<br><br>\n",
    "![title](./Robot-4.png)\n",
    "\n",
    "\n",
    "결과적으로 '위로 움직이는' 행동이 가장 높은 값을 보여준다. \n",
    "\n",
    "이제 이 결과값을 벨만 방정식에 대입하며 (1,1)의 효용을 얻을 수 있다.\n",
    "$$ U(s_{11}) = -0.04 + 1.0 \\times 0.7456 = 0.7056 $$ \n",
    "\n",
    "이제 파이썬으로 구현해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utility of state (1,1): 0.7056\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def return_state_utility(v, T, u, reward, gamma) :\n",
    "    \"\"\"\n",
    "    Return the state utility.\n",
    "    \n",
    "    @param v the state vector\n",
    "    @param T transition matrix\n",
    "    @param u utility vector\n",
    "    @param reward for that state\n",
    "    @param gamma discount factor\n",
    "    @return the utility of the state\n",
    "    \"\"\"\n",
    "    \n",
    "    action_array = np.zeros(4)\n",
    "    for action in range(0,4) :\n",
    "        action_array[action] = np.sum(np.multiply(u, np.dot(v, T[:,:,action])))\n",
    "    return reward + gamma * np.max(action_array)\n",
    "\n",
    "def main() :\n",
    "    #Starting state vector\n",
    "    #The agent starts from (1,1)\n",
    "    \n",
    "    v = np.array([[0.0, 0.0, 0.0, 0.0,\n",
    "                  0.0, 0.0, 0.0, 0.0,\n",
    "                  1.0, 0.0, 0.0, 0.0]])\n",
    "    \n",
    "    # Transition matrix loaded from file\n",
    "    T = np.load(\"T.npy\")\n",
    "    \n",
    "    \n",
    "    #Utility vector\n",
    "    u = np.array([[0.812, 0.868, 0.918, 1.0,\n",
    "                  0.762, 0.0, 0.660, -1.0,\n",
    "                  0.705, 0.655, 0.611, 0.388]])\n",
    "    \n",
    "    # Defining the reward for state (1,1)\n",
    "    reward = -0.04\n",
    "    # Assume discount factor = 1\n",
    "    gamma = 1.0\n",
    "    \n",
    "    # Bellman equation to find the utility of state (1,1)\n",
    "    utility_11 = return_state_utility(v, T, u, reward, gamma)\n",
    "    print(\"Utility of state (1,1): \" + str(utility_11))\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\" :\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제까지, 우리는 효용값들을 주어졌다고 가정한 상태에서 위 계산을 시행하였다.\n",
    "하지만 `n`개의 가능한 `state`에 대해서, `n`개의 상응하는 벨만 방정식이 존재하고, 각 방정식은 `n`개의 미지수를 포함한다.\n",
    "\n",
    "선형대수 패키지를 사용하면 풀 수는 있겠지만, 문제는 `max` 연산자 때문에 선형 연산이 불가능하다는 점이다. \n",
    "\n",
    "이 때, **Value Iteration(가치반복) Algorithm**을 사용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Value Iteration Algorithm (가치반복 알고리즘)\n",
    "\n",
    "MDP문제를 풀 때, 벨만 방정식은 'Value Iteration'의 핵심이다. \n",
    "\n",
    "우리의 목표는 각 `state`의 효용(`utility`)를 찾는 것이다. 하지만 앞서 말했다시피, 선형 패키지를 사용할 수 없고, 따라서 반복법을 통해 접근한다.\n",
    "\n",
    "1. 먼저 임의의 초기 값(보통 0)으로 시작한다.\n",
    "2. 그리고 벨만 방정식을 이용해 `state`의 효용을 계산하여 다시 `state`에 할당한다. 이 과정은 **벨만 업데이트(Bellman Update)**라고 부른다.\n",
    "3. 벨만 업데이트를 무한히 반복하다보면 평형 상태에 도달한다. 그 값을 사용하면 된다.\n",
    "    4. 어떻게 평형 상태에 도달했는지 알 수 있을까? -> **Stopping Criteria**를 사용한다.\n",
    "        - 어떠한 `state`의 효용도 많이 바뀌지 않을 때, 우리는 알고리즘을 멈출 수 있다.\n",
    "        - 다음 식을 참고하라.\n",
    "        - 이 결과는 **Contraction Property**의 결과이지만, 여기서 설명하지는 않는다.\n",
    "        \n",
    "        $$ \\vert \\vert U_{k+1} - U_{k} || < \\in \\frac{1-\\gamma}{\\gamma} $$\n",
    "        \n",
    " \n",
    "다시 파이썬으로 구현해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================== FINAL RESULT ==================\n",
      "Iterations: 26\n",
      "Delta: 9.51196868787e-06\n",
      "Gamma: 0.999\n",
      "Epsilon: 0.01\n",
      "===================================================\n",
      "[ 0.80796341  0.86539911  0.91653199  1.        ]\n",
      "[ 0.75696613  0.          0.65836281 -1.        ]\n",
      "[ 0.69968168  0.64881721  0.60471137  0.3814863 ]\n",
      "===================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def return_state_utility(v, T, u, reward, gamma):\n",
    "    \"\"\"Return the state utility.\n",
    "\n",
    "    @param v the value vector\n",
    "    @param T transition matrix\n",
    "    @param u utility vector\n",
    "    @param reward for that state\n",
    "    @param gamma discount factor\n",
    "    @return the utility of the state\n",
    "    \"\"\"\n",
    "    action_array = np.zeros(4)\n",
    "    for action in range(0, 4):\n",
    "        action_array[action] = np.sum(np.multiply(u, np.dot(v, T[:,:,action])))\n",
    "    return reward + gamma * np.max(action_array)\n",
    "\n",
    "def main():\n",
    "    #Change as you want\n",
    "    tot_states = 12\n",
    "    gamma = 0.999 #Discount factor\n",
    "    iteration = 0 #Iteration counter\n",
    "    epsilon = 0.01 #Stopping criteria small value\n",
    "\n",
    "    #List containing the data for each iteation\n",
    "    graph_list = list()\n",
    "\n",
    "    #Transition matrix loaded from file (It is too big to write here)\n",
    "    T = np.load(\"T.npy\")\n",
    "\n",
    "    #Reward vector\n",
    "    r = np.array([-0.04, -0.04, -0.04,  +1.0,\n",
    "                  -0.04,   0.0, -0.04,  -1.0,\n",
    "                  -0.04, -0.04, -0.04, -0.04])    \n",
    "\n",
    "    #Utility vectors\n",
    "    u = np.array([0.0, 0.0, 0.0,  0.0,\n",
    "                   0.0, 0.0, 0.0,  0.0,\n",
    "                   0.0, 0.0, 0.0,  0.0])\n",
    "    u1 = np.array([0.0, 0.0, 0.0,  0.0,\n",
    "                    0.0, 0.0, 0.0,  0.0,\n",
    "                    0.0, 0.0, 0.0,  0.0])\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "        u = u1.copy()\n",
    "        iteration += 1\n",
    "        graph_list.append(u)\n",
    "        for s in range(tot_states):\n",
    "            reward = r[s]\n",
    "            v = np.zeros((1,tot_states))\n",
    "            v[0,s] = 1.0\n",
    "            u1[s] = return_state_utility(v, T, u, reward, gamma)\n",
    "            delta = max(delta, np.abs(u1[s] - u[s])) #Stopping criteria       \n",
    "        if delta < epsilon * (1 - gamma) / gamma:\n",
    "                print(\"=================== FINAL RESULT ==================\")\n",
    "                print(\"Iterations: \" + str(iteration))\n",
    "                print(\"Delta: \" + str(delta))\n",
    "                print(\"Gamma: \" + str(gamma))\n",
    "                print(\"Epsilon: \" + str(epsilon))\n",
    "                print(\"===================================================\")\n",
    "                print(u[0:4])\n",
    "                print(u[4:8])\n",
    "                print(u[8:12])\n",
    "                print(\"===================================================\")\n",
    "                break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./Iterations.png)\n",
    "\n",
    "감마의 값이 1에 수렴할수록 효용에 대한 예측력은 더 정확해진다.\n",
    "\n",
    "하지만 감마가 1이 되어버리면 Stop Criteria에 걸치지 않기 때문에 알고리즘은 무한 루프에 빠진다.\n",
    "\n",
    "\n",
    "이 알고리즘 외에도, 효용 벡터와 그 동시에 최적 규칙을 찾아주는 알고리즘이 있다.\n",
    "<br><br>\n",
    "### The Policy Iteration Algorithm\n",
    "VIA를 통해, 우리는 각 `state`의 효용을 측정할 수 있었다. 하지만 최적 규칙을 평가할 방법은 없었다.\n",
    "\n",
    "여기서는 기대 효용을 최대화시켜 최적 규칙을 찾는 PIA를 소개할 것이다.\n",
    "\n",
    "그 어떠한 규칙도 최적 규칙인 $ \\pi ^* $ 보다 더 큰 보상을 만들지는 못한다. \n",
    "\n",
    "PIA는 반드시 수렴하게 되어있고, 수렴하는 시점의 규칙과 효용 함수가 '최적 규칙'과 '최적 효용 함수'가 된다.\n",
    "\n",
    "1. 우선, 각 `state`에 행동을 할당하는 규칙인 $\\pi$를 정의한다. 여기서 임의의 행동을 할당해도 상관없다.\n",
    "2. 벨만 방정식을 사용하여, 이 규칙의 기대 효용값을 계산한다.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
