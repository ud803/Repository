{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2. 전방향 신경망 학습\n",
    "\n",
    "\n",
    "### 2-1. 패스트푸드 문제\n",
    "\n",
    "딥러닝을 통해 여러가지 문제를 풀 수 있다. 하지만 파라미터 벡터를 알아내는 것이 가장 큰 문제이다. \n",
    "\n",
    "이것은 일반적으로 학습의 과정을 통해 알아낼 수 있고, 그 학습 예제들에 대한 오차를 최소화하기 위해 반복적으로 가중치를 수정할 수 있다.\n",
    "\n",
    "예를 들어, 패스트푸드 식당에서 햄버거, 감자튀김, 탄산음료로 구성된 세트 메뉴를 사고, 세트 메뉴의 단품 개수에 따라 돈을 낸다고 가정하자.\n",
    "\n",
    "단품의 가격이 나와있지는 않다면 하나의 선형 뉴런을 통해 어떻게 해결할 수 있을까?\n",
    "\n",
    "\n",
    "아주 큰 집합의 학습 데이터가 있을 때, 간단한 수식을 사용해 i번째 학습 데이터에서 오차를 최소화하는 출력을 계산할 수 있다.\n",
    "\n",
    "$$ E = \\frac{1}{2}*\\sum_i(t^{i}-y^{i})^2$$\n",
    "\n",
    "하지만 불행히도, 이 생각은 일반화하기 어렵다. 선형 뉴런은 학습시키는 데 제약이 있어서 실제로는 잘 사용되지 않기 때문이다.\n",
    "\n",
    "그리고 시그모이드, tanh, ReLU 같은 비선형 뉴런을 사용하기 시작하는 순간 더는 연립 방정식을 만들 수 없다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2. 경사 하강법\n",
    "\n",
    "문제를 단순화해 모든 학습 예제의 오차를 최소화할 방법을 생각해보자. 선형 뉴런이 두 개의 입력(두 개의 가중치 $w_1$과 $w_2$)만을 갖는다고 하자.\n",
    "\n",
    "그러면 두 가중치로 이루어진 수평 차원과 오차 함수 E로 만들어진 수직 차원의 3차원 공간을 상상할 수 있다. 결국 3차원에서 사발 모양의 곡면을 얻게 된다.\n",
    "\n",
    "이 곡면을 타원 등고선들의 집합으로 편리하게 시각화할 수 있고, 최소 오차는 타원의 중심에 있다. 등고선이 가까울수록 경사는 가파르다.\n",
    "\n",
    "실제로 가장 가파른 하강 방향은 항상 등고선에 대해 수직이고, 이 방향은 **경사(gradient)**라는 벡터로 표현된다.\n",
    "\n",
    "그렇다면 임의이의 한 위치에서 경사를 구해 가장 가파른 경사 방향을 찾을 수 있으며, 해당 방향으로 한 걸음 나아갈 수 있다.\n",
    "\n",
    "그러면 원래 있던 곳보다 최소값에 더 가까운 새로운 위치에 있게 되고, 이 과정을 반복하여 결과적으로 최소 오차 지점에 도달할 수 있다.\n",
    "\n",
    "이 알고리즘을 **경사 하강법(Gradient Descent, GD)** 이라고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradientDescent(data) :\n",
    "    # type(data) == list\n",
    "    learning_rate = 0.05\n",
    "    for i in data :\n",
    "        x = i.x\n",
    "        y = i.y\n",
    "        guess = m * x + b\n",
    "        error = y - guess\n",
    "        m = m + error * x * learning_rate\n",
    "        b = b + error * learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3. 델타 규칙과 학습률\n",
    "\n",
    "패스트푸드 뉴런을 학습시키는 정확한 알고리즘을 도출하기 전에 **하이퍼파라미터(Hyperparameter)**를 먼저 알아보자. \n",
    "\n",
    "신경망에서 정의한 가중치 파라미터 외에도 학습 알고리즘 또한 파라미터가 필요한데, 이를 하이퍼파라미터라 하고 그중 하나가 바로 **학습률(Learning Rate)**이다.\n",
    "\n",
    "실제로는 등고선에 수직으로 이동하는 단계에서 새로운 방향을 재계산하기 전에 얼마나 멀리 나갈지를 결정해야 한다. \n",
    "\n",
    "이 거리는 곡면의 가파른 정도에 의존하는데, 최소값에 가까울수록 앞으로 나가는 거리는 더 짧아진다. 하지만 오차 곡면이 매우 완만하다면 학습에 시간이 오래 걸리고, 종종 경사에 학습률 $\\epsilon$을 곱한다.\n",
    "\n",
    "학습률이 너무 높으면 최소값 근처에서 발산할 가능성이 높고, 너무 낮으면 아주 긴 학습 과정을 감수해야 한다. \n",
    "\n",
    "차후에 학습률을 선택하는 과정을 자동화하는 **학습률 적응(Learning rate adaptation)**을 활용한 다양한 최적화 기법을 배운다.\n",
    "\n",
    "이제 선형 뉴런 학습에 대한 **델타 규칙(Delta rule)**을 도출해보자. 각 가중치를 어떻게 바꿀지 계산하기 위해 경사를 평가한다.\n",
    "\n",
    "경사는 각 가중치에 대한 오차 함수의 편도함수(partial derivative)로 다음 수식을 통해 표현한다.\n",
    "\n",
    "$$ \\Delta w_k = -\\epsilon \\frac{\\partial E}{w_k} = -\\epsilon\\frac{\\partial}{w_k}(\\frac{1}{2}\\sum_t(t^i-y^i)^2) = \\sum_i\\epsilon x_k^i(t^i-y^i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-4. 시그모이드 뉴런의 경사 하강법\n",
    "\n",
    "여기서는 시그모이드 뉴런을 사용하겠다. 이 경우 바이어스 항목을 사용해도 되지만 단순화하기 위해 사용하지 않는다고 가정한다.\n",
    "\n",
    "바이어스는 단지 값이 항상 1인 입력 연결의 가중치라고 가정하면 된다. 다음처럼 입력으로부터 출력 값을 계산하는 로지스틱 뉴런에 의한 동작 방식을 상기해보자.\n",
    "\n",
    "먼저 뉴런은 입력의 가중합인 로짓 z를 계산한다. 그다음 최종 출력인 y를 계산하기 위해 입력 함수에 해당 로짓을 제공한다.\n",
    "\n",
    "$$ z = \\sum_kw_kx_k$$\n",
    "\n",
    "$$ y = \\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "\n",
    "학습을 위해 가중치들에 대한 오차 함수의 경사를 계산하려면 입력과 가중치들에 대한 로짓 함수의 도함수를 다음과 같이 취한다.\n",
    "\n",
    "$$ \\frac{\\partial z}{\\partial w_k} = x_k $$\n",
    "\n",
    "$$ \\frac{\\partial z}{\\partial x_k} = w_k $$\n",
    "\n",
    "가중치를 수정하기 위한 최종 규칙은 다음과 같다.\n",
    "\n",
    "$$ \\Delta W_k = \\sum_i \\epsilon x_k^iy^i(i-y^i)(t^i-y^i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-5. 역전파 알고리즘\n",
    "\n",
    "이제 단순한 한 층의 뉴런들 대신 다층 신경망을 학습시키는 문제를 해결해보자. 이를 위해 1986년 데이비드 등이 개발한 **역전파(backpropagation)** 방식을 사용한다.\n",
    "\n",
    "역전파 알고리즘에서 **은닉 유닛(hidden unit)**이 무엇을 해야 하는지는 알지 못하지만, 은닉층의 활성도를 바꿀 때 얼마나 빨리 오차가 변하는지 계산할 수 있다.\n",
    "\n",
    "궁극적으로 가파른 하강 방향을 찾으려 노력하는데, 문제점은 매우 고차원의 공간에서 문제를 다루게 된다는 것이다. 하나의 예제에 대한 오차 도함수를 계산해보자.\n",
    "\n",
    "각 은닉 유닛은 많은 출력 유닛에 영향을 미친다. \n",
    "\n",
    "우리의 전략은 일종의 동적 프로그래밍인데, \n",
    "1. 은닉 유닛의 한 개 층에 대한 오차 도함수를 얻고, \n",
    "2. 이를 아래층의 활성도에 대한 오차 도함수를 계산하는 데 사용된다.\n",
    "3. 그리고 은닉 유닛의 활성도에 대한 오차 도함수를 찾으면 가중치에 대한 도함수를 구하기가 쉽다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
